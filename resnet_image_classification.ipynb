{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification with ResNet\n",
    "\n",
    "This notebook implements a custom CNN using PyTorch for image classification, featuring:\n",
    "- Custom ResNet architecture\n",
    "- CIFAR-10 dataset training\n",
    "- Streamlit interface\n",
    "- Model explainability with GradCAM\n",
    "- Performance optimizations\n",
    "\n",
    "Author: ShaLese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install torch torchvision streamlit captum gradcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "import torch.multiprocessing as mp\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "# Device agnostic code\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Custom ResNet Block\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1) -> None:\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "# Custom ResNet\n",
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, num_classes: int = 10) -> None:\n",
    "        super().__init__()\n",
    "        self.in_channels = 64\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.layer1 = self.make_layer(64, 2, stride=1)\n",
    "        self.layer2 = self.make_layer(128, 2, stride=2)\n",
    "        self.layer3 = self.make_layer(256, 2, stride=2)\n",
    "        \n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(256, num_classes)\n",
    "\n",
    "    def make_layer(self, out_channels: int, num_blocks: int, stride: int) -> nn.Sequential:\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(ResBlock(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.avg_pool(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Data loading and preprocessing\n",
    "def get_data_loaders(batch_size: int = 128, num_workers: int = 4) -> Tuple[DataLoader, DataLoader]:\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "    ])\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
    "    testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return trainloader, testloader\n",
    "\n",
    "# Training function with timing and progress tracking\n",
    "def train_model(model: nn.Module, trainloader: DataLoader, epochs: int = 10) -> List[float]:\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        start_time = time.time()\n",
    "        \n",
    "        progress_bar = tqdm(trainloader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        for inputs, targets in progress_bar:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': running_loss/len(trainloader)})\n",
    "        \n",
    "        epoch_loss = running_loss / len(trainloader)\n",
    "        losses.append(epoch_loss)\n",
    "        scheduler.step()\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f'Epoch {epoch+1} completed in {epoch_time:.2f} seconds. Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Model evaluation\n",
    "def evaluate_model(model: nn.Module, testloader: DataLoader) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(testloader, desc='Evaluating'):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "    \n",
    "    accuracy = 100. * correct / total\n",
    "    eval_time = time.time() - start_time\n",
    "    return accuracy, eval_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# GradCAM implementation for model explainability\n",
    "class GradCAM:\n",
    "    def __init__(self, model: nn.Module, target_layer: nn.Module):\n",
    "        self.model = model\n",
    "        self.target_layer = target_layer\n",
    "        self.gradients = None\n",
    "        self.features = None\n",
    "        \n",
    "        self.target_layer.register_forward_hook(self.save_features)\n",
    "        self.target_layer.register_backward_hook(self.save_gradients)\n",
    "    \n",
    "    def save_features(self, module: nn.Module, input: torch.Tensor, output: torch.Tensor) -> None:\n",
    "        self.features = output.detach()\n",
    "    \n",
    "    def save_gradients(self, module: nn.Module, grad_input: torch.Tensor, grad_output: torch.Tensor) -> None:\n",
    "        self.gradients = grad_output[0].detach()\n",
    "    \n",
    "    def generate_cam(self, input_image: torch.Tensor, target_class: int) -> np.ndarray:\n",
    "        self.model.eval()\n",
    "        output = self.model(input_image)\n",
    "        \n",
    "        if target_class is None:\n",
    "            target_class = output.argmax(dim=1).item()\n",
    "        \n",
    "        self.model.zero_grad()\n",
    "        output[0, target_class].backward()\n",
    "        \n",
    "        pooled_gradients = torch.mean(self.gradients, dim=[0, 2, 3])\n",
    "        for i in range(pooled_gradients.shape[0]):\n",
    "            self.features[:, i, :, :] *= pooled_gradients[i]\n",
    "            \n",
    "        heatmap = torch.mean(self.features, dim=1).squeeze()\n",
    "        heatmap = F.relu(heatmap)\n",
    "        heatmap /= torch.max(heatmap)\n",
    "        \n",
    "        return heatmap.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Main execution\n",
    "def main():\n",
    "    print(f'Using device: {device}')\n",
    "    \n",
    "    # Initialize model and data\n",
    "    model = CustomResNet()\n",
    "    trainloader, testloader = get_data_loaders()\n",
    "    \n",
    "    # Train the model\n",
    "    print('Starting training...')\n",
    "    start_time = time.time()\n",
    "    losses = train_model(model, trainloader)\n",
    "    training_time = time.time() - start_time\n",
    "    print(f'Training completed in {training_time:.2f} seconds')\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy, eval_time = evaluate_model(model, testloader)\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    print(f'Evaluation completed in {eval_time:.2f} seconds')\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), 'resnet_cifar10.pth')\n",
    "    \n",
    "    return model, losses\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model, losses = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Streamlit interface code (save as app.py when deploying)\n",
    "'''\n",
    "import streamlit as st\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from model import CustomResNet  # assuming model is saved in model.py\n",
    "\n",
    "# Load the model\n",
    "def load_model():\n",
    "    model = CustomResNet()\n",
    "    model.load_state_dict(torch.load('resnet_cifar10.pth', map_location=torch.device('cpu')))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Preprocess image\n",
    "def preprocess_image(image):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "    return transform(image).unsqueeze(0)\n",
    "\n",
    "# Main Streamlit app\n",
    "def main():\n",
    "    st.title('Image Classification with ResNet')\n",
    "    st.write('Upload an image for classification')\n",
    "    \n",
    "    uploaded_file = st.file_uploader('Choose an image...', type=['jpg', 'jpeg', 'png'])\n",
    "    \n",
    "    if uploaded_file is not None:\n",
    "        # Display the uploaded image\n",
    "        image = Image.open(uploaded_file)\n",
    "        st.image(image, caption='Uploaded Image', use_column_width=True)\n",
    "        \n",
    "        # Make prediction\n",
    "        model = load_model()\n",
    "        processed_image = preprocess_image(image)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            start_time = time.time()\n",
    "            outputs = model(processed_image)\n",
    "            prediction_time = time.time() - start_time\n",
    "            \n",
    "            probabilities = torch.nn.functional.softmax(outputs, dim=1)[0]\n",
    "            predicted_class = torch.argmax(probabilities).item()\n",
    "            \n",
    "            # Generate GradCAM visualization\n",
    "            gradcam = GradCAM(model, model.layer3[-1])\n",
    "            cam = gradcam.generate_cam(processed_image, predicted_class)\n",
    "            \n",
    "            # Display results\n",
    "            st.write(f'Predicted class: {predicted_class}')\n",
    "            st.write(f'Confidence: {probabilities[predicted_class]:.2f}')\n",
    "            st.write(f'Prediction time: {prediction_time:.3f} seconds')\n",
    "            \n",
    "            # Display GradCAM\n",
    "            st.image(cam, caption='GradCAM Visualization', use_column_width=True)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
